{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL_0UKJOj1YP"
      },
      "outputs": [],
      "source": [
        "# import all necessary modules\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import sympy as sym # sympy to compute the partial derivatives\n",
        "\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuu117ieVIj0"
      },
      "source": [
        "Given the function:\n",
        "\n",
        "\\begin{align}\n",
        "f(x,y) = e^x sin(y) + y^2\n",
        "\\end{align}\n",
        "\n",
        "We need to find the the global minima, analyticall we can use partial derivatives\n",
        "\\begin{align}\n",
        "\\frac{\\partial{f(x,y)}}{\\partial{(x,y)}} = \\frac{\\partial{(e^x sin(y) + y^2)}}{\\partial{(x,y)}} \\\\\n",
        "\\frac{\\partial{f(x,y)}}{\\partial{(x)}} = \\frac{\\partial{(e^x sin(y) + y^2)}}{\\partial{(x)}} \\\\\n",
        "\\frac{\\partial{f(x,y)}}{\\partial{(x)}} = e^x sin(y) \\\\\n",
        "\\frac{\\partial{f(x,y)}}{\\partial{(y)}} = \\frac{\\partial{(e^x sin(y) + y^2)}}{\\partial{(y)}} \\\\\n",
        "\\frac{\\partial{f(x,y)}}{\\partial{(y)}} = e^x cos(y) + 2y\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0w52qmG_bj1w"
      },
      "outputs": [],
      "source": [
        "# the \"peaks\" function\n",
        "def f(x,y):\n",
        "    # expand to a 2D mesh\n",
        "    x,y = np.meshgrid(x,y)\n",
        "    z = (math.e**x)*np.sin(y) \\\n",
        "    + y**2\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFLOlXhVbj5A"
      },
      "outputs": [],
      "source": [
        "# create the landscape\n",
        "x = np.linspace(-3,3,201)\n",
        "y = np.linspace(-3,3,201)\n",
        "\n",
        "Z = f(x,y)\n",
        "\n",
        "plt.imshow(Z,extent=[x[0],x[-1],y[0],y[-1]],vmin=-5,vmax=5,origin='lower')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guL7sYzwbj_q"
      },
      "outputs": [],
      "source": [
        "# Define the symbols\n",
        "sx, sy = sym.symbols('sx sy')\n",
        "\n",
        "# Define the symbolic expression using sympy functions\n",
        "sZ = (sym.exp(sx)) * sym.sin(sy) + sy**2\n",
        "\n",
        "# Create functions from the sympy-computed derivatives\n",
        "df_x = sym.lambdify((sx, sy), sym.diff(sZ, sx), 'sympy')\n",
        "df_y = sym.lambdify((sx, sy), sym.diff(sZ, sy), 'sympy')\n",
        "\n",
        "# Evaluate the derivative at a specific point\n",
        "result_x = df_x(1, 1)\n",
        "result_y = df_y(1, 1)\n",
        "\n",
        "print(f\"df_x(1, 1) = {result_x}\")\n",
        "print(f\"df_y(1, 1) = {result_y}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56kT6HtSWnc1"
      },
      "outputs": [],
      "source": [
        "df_x(10,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo3H8RpgiUN9",
        "outputId": "2ec9042a-f83a-4b74-c830-db9b3fe5e355",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2822391165.py:16: RuntimeWarning: overflow encountered in multiply\n",
            "  localmin = localmin - learning_rate*grad  # add _ or [:] to change a variable in-place\n",
            "/tmp/ipython-input-2822391165.py:17: RuntimeWarning: overflow encountered in cast\n",
            "  trajectory[i,:] = localmin\n"
          ]
        }
      ],
      "source": [
        "# random starting point (uniform between -2 and +2)\n",
        "\n",
        "localmin = np.random.rand(2) * 4 - 2  # random starting point [-2,2]\n",
        "startpnt = localmin.copy()            # make a copy for plotting\n",
        "\n",
        "# learning parameters\n",
        "learning_rate = 0.034\n",
        "training_epochs = 200\n",
        "\n",
        "# run through training\n",
        "trajectory = np.zeros((training_epochs,2))\n",
        "for i in range(training_epochs):\n",
        "    grad = np.array([ df_x(localmin[0],localmin[1]),\n",
        "                    df_y(localmin[0],localmin[1])\n",
        "                  ])\n",
        "    localmin = localmin - learning_rate*grad  # add _ or [:] to change a variable in-place\n",
        "    trajectory[i,:] = localmin\n",
        "\n",
        "\n",
        "print(localmin)\n",
        "print(startpnt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRrjCTqFbkCo"
      },
      "outputs": [],
      "source": [
        "# let's have a look!\n",
        "plt.imshow(Z,extent=[x[0],x[-1],y[0],y[-1]],vmin=-5,vmax=5,origin='lower')\n",
        "plt.plot(startpnt[0],startpnt[1],'bs')\n",
        "plt.plot(localmin[0],localmin[1],'ro')\n",
        "plt.plot(trajectory[:,0],trajectory[:,1],'r')\n",
        "plt.legend(['rnd start','local min'])\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvGE12fEiui7"
      },
      "source": [
        "# Programming Assignments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m54Y_kYXiwO9"
      },
      "source": [
        "1. Experiment with the values of the learning rate and epochs using a constant value of the initial weights, and describe the results.\n",
        "2. Experiment with different values of the initial weights at constant values of learning rate and epoch, describe your results.\n",
        "3. Redo the code to perform Gradient Ascent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHBx2c0tWnc2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}